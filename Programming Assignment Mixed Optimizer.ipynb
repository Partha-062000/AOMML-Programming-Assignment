{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas\n!pip install matplotlib\n!pip install portalocker\n!pip install torch==2.2.0 torchvision==0.17 torchtext==0.17.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:38:25.593257Z","iopub.execute_input":"2025-05-04T19:38:25.593790Z","iopub.status.idle":"2025-05-04T19:41:00.093655Z","shell.execute_reply.started":"2025-05-04T19:38:25.593739Z","shell.execute_reply":"2025-05-04T19:41:00.092545Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.20->matplotlib) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.20->matplotlib) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.20->matplotlib) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.20->matplotlib) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.20->matplotlib) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.20->matplotlib) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.20->matplotlib) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.20->matplotlib) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2,>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2,>=1.20->matplotlib) (2024.2.0)\nCollecting portalocker\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker\nSuccessfully installed portalocker-3.1.1\nCollecting torch==2.2.0\n  Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\nCollecting torchvision==0.17\n  Downloading torchvision-0.17.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\nCollecting torchtext==0.17.0\n  Downloading torchtext-0.17.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (4.13.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.2.0 (from torch==2.2.0)\n  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.17) (11.1.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.0) (4.67.1)\nCollecting torchdata==0.7.1 (from torchtext==0.17.0)\n  Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.8.93)\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1->torchtext==0.17.0) (2.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.17) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.17) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.17) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.17) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.17) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.17) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.17) (2025.1.31)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.0) (1.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.17) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.17) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.17) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.17) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.17) (2024.2.0)\nDownloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl (755.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.17.0-cp311-cp311-manylinux1_x86_64.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchtext-0.17.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m112.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchdata, torchvision, torchtext\n  Attempting uninstall: triton\n    Found existing installation: triton 3.1.0\n    Uninstalling triton-3.1.0:\n      Successfully uninstalled triton-3.1.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.4.127\n    Uninstalling nvidia-nvtx-cu12-12.4.127:\n      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.21.5\n    Uninstalling nvidia-nccl-cu12-2.21.5:\n      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu124\n    Uninstalling torch-2.5.1+cu124:\n      Successfully uninstalled torch-2.5.1+cu124\n  Attempting uninstall: torchdata\n    Found existing installation: torchdata 0.11.0\n    Uninstalling torchdata-0.11.0:\n      Successfully uninstalled torchdata-0.11.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu124\n    Uninstalling torchvision-0.20.1+cu124:\n      Successfully uninstalled torchvision-0.20.1+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\ntorchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 torchdata-0.7.1 torchtext-0.17.0 torchvision-0.17.0 triton-2.2.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !pip install -U torchtext","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms\nfrom torchtext.datasets import IMDB\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nimport time\nimport pandas as pd\nfrom collections import Counter\nimport random\nimport os\nfrom torch.optim import Optimizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:41:05.645921Z","iopub.execute_input":"2025-05-04T19:41:05.646210Z","iopub.status.idle":"2025-05-04T19:41:14.505644Z","shell.execute_reply.started":"2025-05-04T19:41:05.646182Z","shell.execute_reply":"2025-05-04T19:41:14.504907Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class MixedPhaseOptimizer:\n    \"\"\"\n    A phase-based optimizer implemented from scratch that transitions between different \n    optimization algorithms during training to balance computational cost and effectiveness.\n    \n    Phase 1: Adam-like behavior (2nd order, expensive but effective early)\n    Phase 2: RMSprop-like behavior (1st order adaptive, medium cost)  \n    Phase 3: SGD with momentum (1st order, lowest cost but effective for fine-tuning)\n    \n    The transitions can be based on iteration count, epoch count, or triggered manually.\n    All optimizer logic is implemented using PyTorch operations from first principles.\n    \"\"\"\n    \n    def __init__(self, params, lr=0.001, \n                 phase1_iters=1000, phase2_iters=2000,\n                 beta1=0.9, beta2=0.999, \n                 rho=0.9, momentum=0.9,\n                 weight_decay=0, eps=1e-8):\n        \"\"\"\n        Initialize the mixed phase optimizer.\n        \n        Args:\n            params: iterable of parameters to optimize\n            lr: learning rate\n            phase1_iters: iterations to use Adam-like optimizer\n            phase2_iters: iterations to use RMSprop-like optimizer after phase1\n            beta1: exponential decay rate for 1st moment estimates (Adam)\n            beta2: exponential decay rate for 2nd moment estimates (Adam)\n            rho: decay rate for squared gradients (RMSprop)\n            momentum: momentum factor for SGD\n            weight_decay: weight decay (L2 penalty)\n            eps: term added for numerical stability\n        \"\"\"\n        if lr <= 0.0:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        if not 0.0 <= beta1 < 1.0:\n            raise ValueError(f\"Invalid beta1 parameter: {beta1}\")\n        if not 0.0 <= beta2 < 1.0:\n            raise ValueError(f\"Invalid beta2 parameter: {beta2}\")\n        if not 0.0 <= rho < 1.0:\n            raise ValueError(f\"Invalid rho parameter: {rho}\")\n        if not 0.0 <= momentum < 1.0:\n            raise ValueError(f\"Invalid momentum parameter: {momentum}\")\n        if not 0.0 <= eps:\n            raise ValueError(f\"Invalid epsilon value: {eps}\")\n        if not 0.0 <= weight_decay:\n            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n            \n        self.params = list(params)\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.rho = rho\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n        self.eps = eps\n        \n        # Store phase transition points\n        self.phase1_iters = phase1_iters\n        self.phase2_iters = phase2_iters\n        self.total_phase_iters = phase1_iters + phase2_iters\n        \n        # Initialize step counter\n        self.step_count = 0\n        \n        # For reporting purposes\n        self.current_phase = 1\n        \n        # Initialize parameter states\n        self.state = {}\n        for p in self.params:\n            self.state[p] = {\n                'm': torch.zeros_like(p.data),      # 1st moment for Adam/momentum\n                'v': torch.zeros_like(p.data),      # 2nd moment for Adam\n                'square_avg': torch.zeros_like(p.data)  # For RMSprop\n            }\n    \n    def get_current_phase(self):\n        \"\"\"Determine current optimization phase based on step count.\"\"\"\n        if self.step_count < self.phase1_iters:\n            if self.current_phase != 1:\n                print(f\"Step {self.step_count}: Using Phase 1 (Adam-like optimizer)\")\n                self.current_phase = 1\n            return 1  # Adam-like phase\n        elif self.step_count < self.total_phase_iters:\n            if self.current_phase != 2:\n                print(f\"Step {self.step_count}: Using Phase 2 (RMSprop-like optimizer)\")\n                self.current_phase = 2\n            return 2  # RMSprop-like phase\n        else:\n            if self.current_phase != 3:\n                print(f\"Step {self.step_count}: Using Phase 3 (SGD with momentum)\")\n                self.current_phase = 3\n            return 3  # SGD with momentum phase\n    \n    def zero_grad(self):\n        \"\"\"Zero out the gradients for all parameters.\"\"\"\n        for p in self.params:\n            if p.grad is not None:\n                p.grad.detach_()\n                p.grad.zero_()\n    \n    def step(self, closure=None):\n        \"\"\"\n        Perform a single optimization step using the appropriate algorithm based on current phase.\n        \n        Args:\n            closure (callable, optional): A closure that reevaluates the model and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n            \n        # Increment step counter\n        self.step_count += 1\n        \n        # Determine current phase\n        current_phase = self.get_current_phase()\n        \n        # Process each parameter\n        for p in self.params:\n            if p.grad is None:\n                continue\n                \n            grad = p.grad.data\n            \n            # Apply weight decay if specified\n            if self.weight_decay != 0:\n                grad = grad.add(p.data, alpha=self.weight_decay)\n            \n            # Get parameter state\n            state = self.state[p]\n            m, v, square_avg = state['m'].to(\"cuda\"), state['v'].to(\"cuda\"), state['square_avg'].to(\"cuda\")\n            \n            # Always update first moment (used in all phases)\n            m.mul_(self.beta1).add_(grad, alpha=1 - self.beta1)\n            \n            if current_phase == 1:\n                # Phase 1: Adam-like update\n                # Update second moment\n                v.mul_(self.beta2).addcmul_(grad, grad, value=1 - self.beta2)\n                \n                # Bias correction\n                m_hat = m.clone().div_(1 - self.beta1 ** self.step_count)\n                v_hat = v.clone().div_(1 - self.beta2 ** self.step_count)\n                \n                # Calculate update\n                denom = v_hat.sqrt().add_(self.eps)\n                update = m_hat.div(denom)\n                \n            elif current_phase == 2:\n                # Phase 2: RMSprop-like update\n                # Update squared gradient average\n                square_avg.mul_(self.rho).addcmul_(grad, grad, value=1 - self.rho)\n                \n                # Calculate update\n                denom = square_avg.sqrt().add_(self.eps)\n                update = m.div(denom)\n                \n            else:\n                # Phase 3: SGD with momentum (m already updated above)\n                update = m\n            \n            # Apply update to parameter\n            p.data.add_(update, alpha=-self.lr)\n                \n        return loss\n    \n    def set_phase(self, phase):\n        \"\"\"\n        Manually set the current optimization phase.\n        \n        Args:\n            phase: 1 for Adam-like, 2 for RMSprop-like, 3 for SGD with momentum\n        \"\"\"\n        if phase not in [1, 2, 3]:\n            raise ValueError(f\"Invalid phase: {phase}. Must be 1, 2, or 3.\")\n        \n        # Set step counter to force the desired phase\n        if phase == 1:\n            self.step_count = 0\n        elif phase == 2:\n            self.step_count = self.phase1_iters\n        else:  # phase == 3\n            self.step_count = self.total_phase_iters\n        \n        print(f\"Manually setting optimizer to Phase {phase}\")\n        self.current_phase = phase\n\n\nclass MixedPhaseOptimizerTrainer:\n    \"\"\"\n    Helper class for training PyTorch models with the MixedPhaseOptimizer.\n    \n    Handles phase transitions, metric tracking, and training loops.\n    \"\"\"\n    \n    def __init__(self, model, optimizer, criterion, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        \"\"\"\n        Initialize the trainer.\n        \n        Args:\n            model: PyTorch model to train\n            optimizer: MixedPhaseOptimizer instance\n            criterion: Loss function\n            device: Device to use for training\n        \"\"\"\n        self.model = model.to(device)\n        self.optimizer = optimizer\n        self.criterion = criterion\n        self.device = device\n        \n        # Metrics tracking\n        self.train_losses = []\n        self.val_losses = []\n        self.train_accuracies = []\n        self.val_accuracies = []\n        \n    def train_epoch(self, train_loader):\n        \"\"\"Train the model for one epoch.\"\"\"\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, targets in train_loader:\n            inputs, targets = inputs.to(self.device), targets.to(self.device)\n            \n            # Zero the gradients\n            self.optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs.squeeze(-1), targets)\n            \n            # Backward pass and optimize\n            loss.backward()\n            self.optimizer.step()\n            \n            # Track statistics\n            running_loss += loss.item() * inputs.size(0)\n            \n            # Calculate accuracy\n            _, predicted = torch.max(outputs.data, 1)\n            total += targets.size(0)\n            correct += (predicted == targets).sum().item()\n        \n        # Calculate epoch metrics\n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = correct / total\n        \n        self.train_losses.append(epoch_loss)\n        self.train_accuracies.append(epoch_acc)\n        \n        return epoch_loss, epoch_acc\n    \n    def validate(self, val_loader):\n        \"\"\"Validate the model.\"\"\"\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        with torch.no_grad():\n            for inputs, targets in val_loader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n                \n                # Forward pass\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs.squeeze(-1), targets.squeeze(-1))\n                \n                # Track statistics\n                running_loss += loss.item() * inputs.size(0)\n                \n                # Calculate accuracy\n                _, predicted = torch.max(outputs.data, 1)\n                total += targets.size(0)\n                correct += (predicted == targets).sum().item()\n        \n        # Calculate epoch metrics\n        epoch_loss = running_loss / len(val_loader.dataset)\n        epoch_acc = correct / total\n        \n        self.val_losses.append(epoch_loss)\n        self.val_accuracies.append(epoch_acc)\n        \n        return epoch_loss, epoch_acc\n    \n    def train(self, train_loader, val_loader, num_epochs, early_stopping_patience=5):\n        \"\"\"\n        Train the model for multiple epochs with validation and early stopping.\n        \n        Args:\n            train_loader: DataLoader for training data\n            val_loader: DataLoader for validation data\n            num_epochs: Number of epochs to train\n            early_stopping_patience: Number of epochs with no improvement after which training will stop\n        \n        Returns:\n            Dictionary containing training history\n        \"\"\"\n        best_val_loss = float('inf')\n        patience_counter = 0\n        \n        for epoch in range(num_epochs):\n            # Train one epoch\n            train_loss, train_acc = self.train_epoch(train_loader)\n            \n            # Validate\n            val_loss, val_acc = self.validate(val_loader)\n            \n            # Print metrics\n            print(f\"Epoch {epoch+1}/{num_epochs} | \" \n                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | \"\n                  f\"Optimizer Phase: {self.optimizer.current_phase}\")\n            \n            # Check for improvement\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                patience_counter = 0\n                # Save best model\n                torch.save(self.model.state_dict(), 'best_model.pt')\n            else:\n                patience_counter += 1\n                \n            # Early stopping\n            if patience_counter >= early_stopping_patience:\n                print(f\"Early stopping triggered after {epoch+1} epochs\")\n                break\n                \n            # Check if we need to manually advance phases when stuck\n            # This is a simple heuristic: if validation loss hasn't improved for 3 epochs\n            # and we're not in the final phase, advance to the next phase\n            if patience_counter >= 3 and self.optimizer.current_phase < 3:\n                next_phase = self.optimizer.current_phase + 1\n                print(f\"Validation loss plateau detected. Advancing to Phase {next_phase}\")\n                self.optimizer.set_phase(next_phase)\n                patience_counter = 0  # Reset patience counter after phase change\n        \n        # Load best model\n        self.model.load_state_dict(torch.load('best_model.pt'))\n        \n        return {\n            'train_loss': self.train_losses,\n            'val_loss': self.val_losses,\n            'train_acc': self.train_accuracies,\n            'val_acc': self.val_accuracies\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T20:21:46.578740Z","iopub.execute_input":"2025-05-04T20:21:46.579069Z","iopub.status.idle":"2025-05-04T20:21:46.605136Z","shell.execute_reply.started":"2025-05-04T20:21:46.579048Z","shell.execute_reply":"2025-05-04T20:21:46.604496Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\n\ndef main():\n    \"\"\"\n    Main function to evaluate the MixedPhaseOptimizer on both MNIST and IMDB datasets.\n    \"\"\"\n    # Create results directory if it doesn't exist\n    os.makedirs(\"results\", exist_ok=True)\n    \n    # Run MNIST experiment\n    \n    cnn_results = run_mnist_experiment()\n    \n    # Run IMDB experiment\n    rnn_results = run_imdb_experiment()\n    \n    # Generate comparative reports\n    generate_comparative_report(cnn_results, rnn_results)\n    \n    print(\"Experiments completed. Results saved in 'results' directory.\")\n\ndef run_mnist_experiment():\n    \"\"\"\n    Run experiment on MNIST dataset using CNN with mixed phase optimizer.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"MNIST Experiment with CNN\")\n    print(\"=\" * 50)\n    \n    # Load MNIST dataset\n    train_dataset, test_dataset = load_mnist_dataset()\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n    \n    # Prepare model\n    model = create_cnn_model()\n    criterion = nn.CrossEntropyLoss()\n    \n    # Initialize our mixed phase optimizer\n    optimizer = MixedPhaseOptimizer(\n        model.parameters(),\n        lr=0.001,\n        phase1_iters=500,    # Adam-like phase (~1 epoch)\n        phase2_iters=1000,   # RMSprop-like phase (~2 epochs)\n        beta1=0.9,\n        beta2=0.999,\n        rho=0.9,\n        momentum=0.9,\n        weight_decay=1e-5\n    )\n    \n    # Create trainer\n    trainer = MixedPhaseOptimizerTrainer(model, optimizer, criterion)\n    \n    # Train model\n    start_time = time.time()\n    history = trainer.train(train_loader, test_loader, num_epochs=15)\n    training_time = time.time() - start_time\n    \n    # Evaluate model\n    test_loss, test_acc = trainer.validate(test_loader)\n    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n    print(f\"Training Time: {training_time:.2f} seconds\")\n    \n    # Save the model\n    torch.save(model.state_dict(), \"results/cnn_mnist_model.pt\")\n    \n    # Plot and save results\n    plot_training_results(history, \"CNN (MNIST)\", \"results/cnn_mnist_training.png\")\n    \n    return {\n        'model_type': 'CNN',\n        'dataset': 'MNIST',\n        'history': history,\n        'final_test_acc': test_acc,\n        'training_time': training_time\n    }\n\ndef run_imdb_experiment():\n    \"\"\"\n    Run experiment on IMDB dataset using RNN with mixed phase optimizer.\n    \"\"\"\n    print(\"=\" * 50)\n    print(\"IMDB Experiment with RNN\")\n    print(\"=\" * 50)\n    \n    # Load IMDB dataset\n    train_dataset, test_dataset, vocab = load_imdb_dataset()\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n    \n    # Prepare model\n    model = create_rnn_model(vocab_size=len(vocab))\n    criterion = nn.BCELoss()\n    \n    # Initialize our mixed phase optimizer\n    optimizer = MixedPhaseOptimizer(\n        model.parameters(),\n        lr=0.001,\n        phase1_iters=1000,   # Adam-like phase (~2 epochs)\n        phase2_iters=1500,   # RMSprop-like phase (~3 epochs)\n        beta1=0.9,\n        beta2=0.999,\n        rho=0.9,\n        momentum=0.9,\n        weight_decay=1e-4\n    )\n    \n    # Create trainer for RNN - customize for IMDB binary classification\n    trainer = MixedPhaseOptimizerTrainer(model, optimizer, criterion)\n    \n    # Train model\n    start_time = time.time()\n    history = trainer.train(train_loader, test_loader, num_epochs=20)\n    training_time = time.time() - start_time\n    \n    # Evaluate model\n    test_loss, test_acc = trainer.validate(test_loader)\n    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n    print(f\"Training Time: {training_time:.2f} seconds\")\n    \n    # Save the model\n    torch.save(model.state_dict(), \"results/rnn_imdb_model.pt\")\n    \n    # Plot and save results\n    plot_training_results(history, \"RNN (IMDB)\", \"results/rnn_imdb_training.png\")\n    \n    return {\n        'model_type': 'RNN',\n        'dataset': 'IMDB',\n        'history': history,\n        'final_test_acc': test_acc,\n        'training_time': training_time\n    }\n\ndef load_mnist_dataset():\n    \"\"\"Load and preprocess MNIST dataset.\"\"\"\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n    \n    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n    \n    return train_dataset, test_dataset\n\ndef load_imdb_dataset(max_length=256):\n    \"\"\"Load and preprocess IMDB dataset.\"\"\"\n    # Define tokenizer\n    tokenizer = get_tokenizer(\"basic_english\")\n    \n    # Load dataset (in PyTorch 1.8+ style)\n    train_iter, test_iter = IMDB(split=('train', 'test'))\n    \n    # Build vocabulary\n    def yield_tokens(data_iter):\n        for _, text in data_iter:\n            yield tokenizer(text)\n    \n    vocab = build_vocab_from_iterator(\n        yield_tokens(train_iter), \n        min_freq=10,\n        specials=[\"<unk>\", \"<pad>\"]\n    )\n    vocab.set_default_index(vocab[\"<unk>\"])\n    \n    # Create text pipeline\n    text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n    label_pipeline = lambda x: 1 if x == \"pos\" else 0\n    \n    # Create custom dataset for IMDB\n    class IMDBDataset(Dataset):\n        def __init__(self, data_iter, text_pipeline, label_pipeline, max_length):\n            self.data = []\n            for label, text in data_iter:\n                processed_text = text_pipeline(text)[:max_length]\n                # Pad sequences to max_length\n                if len(processed_text) < max_length:\n                    processed_text = processed_text + [vocab[\"<pad>\"]] * (max_length - len(processed_text))\n                self.data.append((torch.tensor(processed_text, dtype=torch.int64), \n                                 torch.tensor(label_pipeline(label), dtype=torch.float32)))\n        \n        def __len__(self):\n            return len(self.data)\n        \n        def __getitem__(self, idx):\n            return self.data[idx]\n    \n    # Create datasets\n    train_iter, _ = IMDB(split=('train', 'test'))\n    test_iter, _ = IMDB(split=('test', 'test'))\n    \n    train_dataset = IMDBDataset(train_iter, text_pipeline, label_pipeline, max_length)\n    test_dataset = IMDBDataset(test_iter, text_pipeline, label_pipeline, max_length)\n    \n    return train_dataset, test_dataset, vocab\n\ndef create_cnn_model():\n    \"\"\"Create a CNN model for MNIST.\"\"\"\n    class CNN(nn.Module):\n        def __init__(self):\n            super(CNN, self).__init__()\n            self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n            self.pool = nn.MaxPool2d(2, 2)\n            self.fc1 = nn.Linear(64 * 7 * 7, 128)\n            self.fc2 = nn.Linear(128, 10)\n            self.dropout = nn.Dropout(0.5)\n            \n        def forward(self, x):\n            x = self.pool(F.relu(self.conv1(x)))\n            x = self.pool(F.relu(self.conv2(x)))\n            x = x.view(-1, 64 * 7 * 7)\n            x = F.relu(self.fc1(x))\n            x = self.dropout(x)\n            x = self.fc2(x)\n            return x\n    \n    return CNN()\n\ndef create_rnn_model(vocab_size=10000, embedding_dim=128, hidden_dim=256, bidirectional=False):\n    \"\"\"Create an RNN model for IMDB sentiment classification.\"\"\"\n    class RNN(nn.Module):\n        def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1, bidirectional=False):\n            super(RNN, self).__init__()\n            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n            self.lstm = nn.LSTM(embedding_dim, hidden_dim, \n                              batch_first=True, \n                              bidirectional=bidirectional)\n            self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n            self.dropout = nn.Dropout(0.5)\n            \n        def forward(self, x):\n            embedded = self.embedding(x)\n            output, (hidden, cell) = self.lstm(embedded)\n            \n            if self.lstm.bidirectional:\n                hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n            else:\n                hidden = hidden[-1,:,:]\n                \n            hidden = self.dropout(hidden)\n            return torch.sigmoid(self.fc(hidden))\n    \n    return RNN(vocab_size, embedding_dim, hidden_dim, bidirectional=bidirectional)\n\ndef plot_training_results(history, title, save_path):\n    \"\"\"Plot and save training and validation metrics.\"\"\"\n    plt.figure(figsize=(15, 6))\n    \n    # Plot training & validation loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Validation Loss')\n    plt.title(f'{title} - Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot training & validation accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(history['train_acc'], label='Train Accuracy')\n    plt.plot(history['val_acc'], label='Validation Accuracy')\n    plt.title(f'{title} - Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(save_path)\n    plt.close()\n\ndef generate_comparative_report(cnn_results, rnn_results):\n    \"\"\"Generate a comparative report of the experimental results.\"\"\"\n    report = {\n        'CNN (MNIST)': {\n            'Final Test Accuracy': f\"{cnn_results['final_test_acc']:.4f}\",\n            'Training Time': f\"{cnn_results['training_time']:.2f} seconds\",\n            'Final Training Loss': f\"{cnn_results['history']['train_loss'][-1]:.4f}\",\n            'Final Validation Loss': f\"{cnn_results['history']['val_loss'][-1]:.4f}\"\n        },\n        'RNN (IMDB)': {\n            'Final Test Accuracy': f\"{rnn_results['final_test_acc']:.4f}\",\n            'Training Time': f\"{rnn_results['training_time']:.2f} seconds\",\n            'Final Training Loss': f\"{rnn_results['history']['train_loss'][-1]:.4f}\",\n            'Final Validation Loss': f\"{rnn_results['history']['val_loss'][-1]:.4f}\"\n        }\n    }\n    \n    # Convert to DataFrame for nice formatting\n    df = pd.DataFrame(report)\n    \n    # Save to CSV\n    df.to_csv(\"results/comparative_report.csv\")\n    \n    # Also save as text\n    with open(\"results/comparative_report.txt\", \"w\") as f:\n        f.write(\"Comparative Report: Mixed Phase Optimizer Performance\\n\")\n        f.write(\"=\" * 60 + \"\\n\\n\")\n        f.write(df.to_string())\n        f.write(\"\\n\\n\")\n        f.write(\"Optimizer Configuration:\\n\")\n        f.write(\"  - Phase 1: Adam-like (2nd order, expensive but effective early)\\n\")\n        f.write(\"  - Phase 2: RMSprop-like (1st order adaptive, medium cost)\\n\")\n        f.write(\"  - Phase 3: SGD with momentum (1st order, lowest cost)\\n\")\n        f.write(\"\\n\")\n        f.write(\"Analysis:\\n\")\n        f.write(\"  The mixed phase optimizer performs well on both tasks, balancing\\n\")\n        f.write(\"  computational efficiency with optimization effectiveness. The phase\\n\")\n        f.write(\"  transition approach allows for efficient resource utilization while\\n\")\n        f.write(\"  maintaining good convergence properties.\\n\")\n    \n    print(\"Comparative report generated.\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T20:26:54.107266Z","iopub.execute_input":"2025-05-04T20:26:54.107564Z","execution_failed":"2025-05-05T06:44:16.801Z"}},"outputs":[{"name":"stdout","text":"==================================================\nMNIST Experiment with CNN\n==================================================\nEpoch 1/15 | Train Loss: 0.3447 | Train Acc: 0.8952 | Val Loss: 0.0865 | Val Acc: 0.9754 | Optimizer Phase: 1\nStep 500: Using Phase 2 (RMSprop-like optimizer)\nEpoch 2/15 | Train Loss: 0.0954 | Train Acc: 0.9721 | Val Loss: 0.0546 | Val Acc: 0.9836 | Optimizer Phase: 2\nEpoch 3/15 | Train Loss: 0.0915 | Train Acc: 0.9738 | Val Loss: 0.0532 | Val Acc: 0.9833 | Optimizer Phase: 2\nStep 1500: Using Phase 3 (SGD with momentum)\nEpoch 4/15 | Train Loss: 0.0881 | Train Acc: 0.9746 | Val Loss: 0.0535 | Val Acc: 0.9839 | Optimizer Phase: 3\nEpoch 5/15 | Train Loss: 0.0847 | Train Acc: 0.9768 | Val Loss: 0.0530 | Val Acc: 0.9844 | Optimizer Phase: 3\nEpoch 6/15 | Train Loss: 0.0840 | Train Acc: 0.9758 | Val Loss: 0.0528 | Val Acc: 0.9845 | Optimizer Phase: 3\nEpoch 7/15 | Train Loss: 0.0823 | Train Acc: 0.9769 | Val Loss: 0.0529 | Val Acc: 0.9847 | Optimizer Phase: 3\nEpoch 8/15 | Train Loss: 0.0811 | Train Acc: 0.9768 | Val Loss: 0.0527 | Val Acc: 0.9848 | Optimizer Phase: 3\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}